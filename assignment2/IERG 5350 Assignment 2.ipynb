{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IERG 5350 Assignment 2: Model-free Tabular RL\n",
    "\n",
    "*2020-2021 Term 1, IERG 5350: Reinforcement Learning. Department of Information Engineering, The Chinese University of Hong Kong. Course Instructor: Professor ZHOU Bolei. Assignment author: PENG Zhenghao, SUN Hao, ZHAN Xiaohang.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Student Name | Student ID |\n",
    "| :----: | :----: |\n",
    "| Wang Wanli | 1155160517 |\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welecome to the assignment 1 of our RL course. The objective of this assignment is for you to understand the classic methods used in tabular reinforcement learning. \n",
    "\n",
    "This assignment has the following sections:\n",
    "\n",
    " - Section 1: Implementation of model-free familiy of algorithms: SARSA, Q-Learning and model-free control. (100 points)\n",
    "\n",
    "You need to go through this self-contained notebook, which contains dozens of **TODOs** in part of the cells and has special `[TODO]` signs. You need to finish all TODOs. Some of them may be easy such as uncommenting a line, some of them may be difficult such as implementing a function. You can find them by searching the `[TODO]` symbol. However, we suggest you to go through the documents step by step, which will give you a better sense of the content.\n",
    "\n",
    "You are encouraged to add more code on extra cells at the end of the each section to investigate the problems you think interesting. At the end of the file, we left a place for you to optionaly write comments (Yes, please give us some either negative or positive rewards so we can keep improving the assignment!). \n",
    "\n",
    "Please report any code bugs to us via Github issues.\n",
    "\n",
    "Before you get start, remember to follow the instruction at https://github.com/cuhkrlcourse/ierg5350-assignment to setup your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: SARSA\n",
    "\n",
    "(30/100 points)\n",
    "\n",
    "You have noticed that in Assignment 1 - Section 2, we always use the function `trainer._get_transitions()` to get the transition dynamics of the environment, while never call `trainer.env.step()` to really interact with the environment. We need to access the internal feature of the environment or have somebody implement `_get_transitions` for us. However, this is not feasible in many cases, especially in some real-world cases like autonomous driving where the transition dynamics is unknown or does not explicitly exist.\n",
    "\n",
    "In this section, we will introduce the Model-free family of algorithms that do not require to know the transitions: they only get information from `env.step(action)`, that collect information by interacting with the environment rather than grab the oracle of the transition dynamics of the environment.\n",
    "\n",
    "We will continue to use the `TabularRLTrainerAbstract` class to implement algorithms, but remember you should not call `trainer._get_transitions()` anymore.\n",
    "\n",
    "We will use a simpler environment `FrozenLakerNotSlippery-v0` to conduct experiments, which has a `4 X 4` grids and is deterministic. This is because, in a model-free setting, it's extremely hard for a random agent to achieve the goal for the first time. To reduce the time of experiments, we choose to use a simpler environment. In the bonus section, you will have the chance to try model-free RL on `FrozenLake8x8-v0` to see what will happen. \n",
    "\n",
    "Now go through each section and start your coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall the idea of SARSA: it's an on-policy TD control method, which has distinct features compared to policy iteration and value iteration:\n",
    "\n",
    "1. Maintain a state-action pair value function $Q(s_t, a_t) = E \\sum_{i=0} \\gamma^{t+i} r_{t+i}$, namely the Q value.\n",
    "2. Do not require to know the internal dynamics of the environment.\n",
    "3. Use an epsilon-greedy policy to balance exploration and exploitation.\n",
    "\n",
    "In SARSA algorithm, we update the state action value (Q value) via TD error: \n",
    "\n",
    "$$TD(s_t, a_t) = r(s_t, a_t) + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)$$\n",
    "\n",
    "where we run the policy to get the next action $a_{t+1} = Policy(s_{t+1})$. (That's why we call SARSA an on-policy algorithm, it use the current policy to evaluate Q value).\n",
    "\n",
    "$$Q^{new}(s_t, a_t) = Q(s_t, a_t) + \\alpha TD(s_t, a_t)$$\n",
    "\n",
    "Wherein $\\alpha$ is the learning rate, a hyper-parameter provided by the user.\n",
    "\n",
    "Now go through the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Import some packages that we need to use\n",
    "from utils import *\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "def _render_helper(env):\n",
    "    env.render()\n",
    "    wait(sleep=0.2)\n",
    "\n",
    "\n",
    "def evaluate(policy, num_episodes, seed=0, env_name='FrozenLake8x8-v0', render=False):\n",
    "    \"\"\"[TODO] You need to implement this function by yourself. It\n",
    "    evaluate the given policy and return the mean episode reward.\n",
    "    We use `seed` argument for testing purpose.\n",
    "    You should pass the tests in the next cell.\n",
    "\n",
    "    :param policy: a function whose input is an interger (observation)\n",
    "    :param num_episodes: number of episodes you wish to run\n",
    "    :param seed: an interger, used for testing.\n",
    "    :param env_name: the name of the environment\n",
    "    :param render: a boolean flag. If true, please call _render_helper\n",
    "    function.\n",
    "    :return: the averaged episode reward of the given policy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create environment (according to env_name, we will use env other than 'FrozenLake8x8-v0')\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Seed the environment\n",
    "    env.seed(seed)\n",
    "\n",
    "    # Build inner loop to run.\n",
    "    # For each episode, do not set the limit.\n",
    "    # Only terminate episode (reset environment) when done = True.\n",
    "    # The episode reward is the sum of all rewards happen within one episode.\n",
    "    # Call the helper function `_render_helper(env)` to render\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        # reset the environment\n",
    "        obs = env.reset()\n",
    "        act = policy(obs)\n",
    "        \n",
    "        ep_reward = 0.0\n",
    "        while True:\n",
    "            next_obs, reward, done, _ = env.step(act)\n",
    "            act = policy(next_obs)\n",
    "            \n",
    "            ep_reward += reward\n",
    "            \n",
    "            if render:\n",
    "                _render_helper(env)\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards.append(ep_reward)\n",
    "\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TabularRLTrainerAbstract:\n",
    "    \"\"\"This is the abstract class for tabular RL trainer. We will inherent the specify \n",
    "    algorithm's trainer from this abstract class, so that we can reuse the codes like\n",
    "    getting the dynamic of the environment (self._get_transitions()) or rendering the\n",
    "    learned policy (self.render()).\"\"\"\n",
    "    \n",
    "    def __init__(self, env_name='FrozenLake8x8-v0', model_based=True):\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(self.env_name)\n",
    "        self.action_dim = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.n\n",
    "        \n",
    "        self.model_based = model_based\n",
    "\n",
    "    def _get_transitions(self, state, act):\n",
    "        \"\"\"Query the environment to get the transition probability,\n",
    "        reward, the next state, and done given a pair of state and action.\n",
    "        We implement this function for you. But you need to know the \n",
    "        return format of this function.\n",
    "        \"\"\"\n",
    "        self._check_env_name()\n",
    "        assert self.model_based, \"You should not use _get_transitions in \" \\\n",
    "            \"model-free algorithm!\"\n",
    "        \n",
    "        # call the internal attribute of the environments.\n",
    "        # `transitions` is a list contain all possible next states and the \n",
    "        # probability, reward, and termination indicater corresponding to it\n",
    "        transitions = self.env.env.P[state][act]\n",
    "\n",
    "        # Given a certain state and action pair, it is possible\n",
    "        # to find there exist multiple transitions, since the \n",
    "        # environment is not deterministic.\n",
    "        # You need to know the return format of this function: a list of dicts\n",
    "        ret = []\n",
    "        for prob, next_state, reward, done in transitions:\n",
    "            ret.append({\n",
    "                \"prob\": prob,\n",
    "                \"next_state\": next_state,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done\n",
    "            })\n",
    "        return ret\n",
    "    \n",
    "    def _check_env_name(self):\n",
    "        assert self.env_name.startswith('FrozenLake')\n",
    "\n",
    "    def print_table(self):\n",
    "        \"\"\"print beautiful table, only work for FrozenLake8X8-v0 env. We \n",
    "        write this function for you.\"\"\"\n",
    "        self._check_env_name()\n",
    "        print_table(self.table)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        raise NotImplementedError(\"You need to override the \"\n",
    "                                  \"Trainer.train() function.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Use the function you write to evaluate current policy.\n",
    "        Return the mean episode reward of 1000 episodes when seed=0.\"\"\"\n",
    "        result = evaluate(self.policy, 1000, env_name=self.env_name)\n",
    "        return result\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Reuse your evaluate function, render current policy \n",
    "        for one episode when seed=0\"\"\"\n",
    "        evaluate(self.policy, 1, render=True, env_name=self.env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class SARSATrainer(TabularRLTrainerAbstract):\n",
    "    def __init__(self,\n",
    "                 gamma=1.0,\n",
    "                 eps=0.1,\n",
    "                 learning_rate=1.0,\n",
    "                 max_episode_length=100,\n",
    "                 env_name='FrozenLake8x8-v0'\n",
    "                 ):\n",
    "        super(SARSATrainer, self).__init__(env_name, model_based=False)\n",
    "\n",
    "        # discount factor\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # epsilon-greedy exploration policy parameter\n",
    "        self.eps = eps\n",
    "\n",
    "        # maximum steps in single episode\n",
    "        self.max_episode_length = max_episode_length\n",
    "\n",
    "        # the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # build the Q table\n",
    "        self.table = np.zeros((self.obs_dim, self.action_dim))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        \"\"\"Implement epsilon-greedy policy\n",
    "\n",
    "        It is a function that take an integer (state / observation)\n",
    "        as input and return an interger (action).\n",
    "        \"\"\"\n",
    "        p = np.random.random_sample()\n",
    "        if p <= self.eps:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            return np.argmax(self.table[obs,:])\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        \n",
    "        self.eps = 1\n",
    "        obs = self.env.reset()\n",
    "        act = self.policy(obs)\n",
    "        \n",
    "        for t in range(self.max_episode_length):\n",
    "            # Gradually reduce epsilon to the minimum: first exploration, then exploitation \n",
    "            if self.eps > 0.1:\n",
    "                self.eps -= 0.01\n",
    "            \n",
    "            next_obs, reward, done, _ = self.env.step(act)\n",
    "            next_act = self.policy(next_obs)\n",
    "\n",
    "            td_error = reward + self.gamma * self.table[next_obs, next_act] - self.table[obs, act]\n",
    "\n",
    "            new_value = self.table[obs, act] + self.learning_rate * td_error\n",
    "\n",
    "            self.table[obs, act] = new_value\n",
    "\n",
    "            obs = next_obs\n",
    "            act = next_act\n",
    "            \n",
    "            if done: \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have finish the SARSA trainer. To make sure your implementation of epsilon-greedy strategy is correct, please run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# set eps = 0 to disable exploration.\n",
    "test_trainer = SARSATrainer(eps=0.0)\n",
    "test_trainer.table.fill(0)\n",
    "\n",
    "# set the Q value of (obs 0, act 3) to 100, so that it should be taken by \n",
    "# policy.\n",
    "test_obs = 0\n",
    "test_act = test_trainer.action_dim - 1\n",
    "test_trainer.table[test_obs][test_act] = 100\n",
    "\n",
    "# assertion\n",
    "assert test_trainer.policy(test_obs) == test_act, \\\n",
    "    \"Your action is wrong! Should be {} but get {}.\".format(\n",
    "        test_act, test_trainer.policy(test_obs))\n",
    "\n",
    "# delete trainer\n",
    "del test_trainer\n",
    "\n",
    "# set eps = 0 to disable exploitation.\n",
    "test_trainer = SARSATrainer(eps=1.0)\n",
    "test_trainer.table.fill(0)\n",
    "\n",
    "act_set = set()\n",
    "for i in range(100):\n",
    "    act_set.add(test_trainer.policy(0))\n",
    "\n",
    "# assertion\n",
    "assert len(act_set) > 1, (\"You sure your uniformaly action selection mechanism\"\n",
    "                          \" is working? You only take action {} when \"\n",
    "                          \"observation is 0, though we run trainer.policy() \"\n",
    "                          \"for 100 times.\".format(act_set))\n",
    "# delete trainer\n",
    "del test_trainer\n",
    "\n",
    "print(\"Policy Test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the next cell to see the result. Note that we use the non-slippery version of a small frozen lake environment `FrozenLakeNotSlipppery-v0` (this is not a ready Gym environment, see `utils.py` for details). This is because, in the model-free setting, it's extremely hard to access the goal for the first time (you should already know that if you watch the agent randomly acting in Assignment 1 - Section 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve TODO\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_sarsa_config = dict(\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    learning_rate=0.1,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.95,\n",
    "    eps=1,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def sarsa(train_config=None):\n",
    "    config = default_sarsa_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = SARSATrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(1, config['max_iteration'] + 1):\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            \n",
    "            # gradually reduce epsilon to 0 before each evalution\n",
    "            trainer.eps = 1e-2 * (config['max_iteration'] - i) / config['evaluate_interval']\n",
    "            \n",
    "            '''\n",
    "            --- THIS IS THE KEY! ---\n",
    "                No matter how well your trainer works, as long as trainer.epsilon > 0,\n",
    "            the evaluation function will always randomly select action occasionally,\n",
    "            which extremely hinders the episode rewards.\n",
    "                With this sentence, the mean episode reward can reach the full score 1.0,\n",
    "            as a comparison, without this sentence, the reward just fluctuate around 0.1.\n",
    "            '''\n",
    "            \n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "    \n",
    "    # to make sure the evalute runs normally...\n",
    "    trainer.eps = 0\n",
    "    \n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.78.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.804.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.838.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.807.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.852.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.856.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.847.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.854.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.875.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.885.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.893.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.933.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.916.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.937.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.962.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.959.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.978.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.825.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.983.\n",
      "[INFO]\tIn 20000 iteration, current mean episode reward is 1.0.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "sarsa_trainer = sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== The state value for action 0 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.020|0.019|0.016|0.035|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.040|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.055|0.054|0.101|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.000|0.208|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 1 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.024|0.000|0.053|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.057|0.000|0.244|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.000|0.193|0.600|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.248|0.578|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 2 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.017|0.027|0.020|0.020|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.000|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.134|0.251|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.501|1.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 3 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.020|0.017|0.027|0.019|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.021|0.000|0.027|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.028|0.000|0.128|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.125|0.148|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "sarsa_trainer.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "sarsa_trainer.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have finished the SARSA algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Q-Learning\n",
    "(30/100 points)\n",
    "\n",
    "Q-learning is an off-policy algorithm who differs from SARSA in the computing of TD error. Instead of running policy to get `next_act` $a'$ and get the TD error by:\n",
    "\n",
    "$r + \\gamma Q(s', a') - Q(s, a)$, \n",
    "\n",
    "in Q-learning we compute the TD error via:\n",
    "\n",
    "$r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$. \n",
    "\n",
    "The reason we call it \"off-policy\" is that the policy involves the computing of next-Q value is not the \"behavior policy\", instead, it is a \"virtural policy\" that always takes the best action given current Q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class QLearningTrainer(TabularRLTrainerAbstract):\n",
    "    def __init__(self,\n",
    "                 gamma=1.0,\n",
    "                 eps=0.1,\n",
    "                 learning_rate=1.0,\n",
    "                 max_episode_length=100,\n",
    "                 env_name='FrozenLake8x8-v0'\n",
    "                 ):\n",
    "        super(QLearningTrainer, self).__init__(env_name, model_based=False)\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # build the Q table\n",
    "        self.table = np.zeros((self.obs_dim, self.action_dim))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        \"\"\"Implement epsilon-greedy policy\n",
    "\n",
    "        It is a function that take an integer (state / observation)\n",
    "        as input and return an interger (action).\n",
    "        \"\"\"\n",
    "        p = np.random.random_sample()\n",
    "        if p <= self.eps:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            return np.argmax(self.table[obs,:])\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        \n",
    "        self.eps = 1\n",
    "        obs = self.env.reset()\n",
    "        \n",
    "        for t in range(self.max_episode_length):\n",
    "            # Gradually reduce epsilon to the minimum: first exploration, then exploitation \n",
    "            if self.eps > 0.1:\n",
    "                self.eps -= 0.01\n",
    "                \n",
    "            act = self.policy(obs)\n",
    "\n",
    "            next_obs, reward, done, _ = self.env.step(act)\n",
    "\n",
    "            td_error = reward + self.gamma * np.max(self.table[next_obs,:]) - self.table[obs, act]\n",
    "\n",
    "            new_value = self.table[obs, act] + self.learning_rate * td_error\n",
    "\n",
    "            self.table[obs, act] = new_value\n",
    "            obs = next_obs\n",
    "            \n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODO\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_q_learning_config = dict(\n",
    "    max_iteration=20000,\n",
    "    max_episode_length=200,\n",
    "    learning_rate=0.05,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.8,\n",
    "    eps=1,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def q_learning(train_config=None):\n",
    "    config = default_q_learning_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = QLearningTrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(1, config['max_iteration'] + 1):\n",
    "        # train the agent\n",
    "        trainer.train()  # [TODO] please uncomment this line\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            \n",
    "            # gradually reduce epsilon to 0 before each evalution\n",
    "            trainer.eps = 1e-2 * (config['max_iteration'] - i) / config['evaluate_interval']\n",
    "            \n",
    "            '''\n",
    "            --- THIS IS THE KEY! ---\n",
    "                No matter how well your trainer works, as long as trainer.epsilon > 0,\n",
    "            the evaluation function will always randomly select action occasionally,\n",
    "            which extremely hinders the episode rewards.\n",
    "                With this sentence, the mean episode reward can reach the full score 1.0,\n",
    "            as a comparison, without this sentence, the reward just fluctuate around 0.1.\n",
    "            '''\n",
    "            \n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "    \n",
    "    # to make sure the evalute runs normally...\n",
    "    trainer.eps = 0\n",
    "    \n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.763.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.81.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.807.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.813.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.852.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.843.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.875.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.872.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.868.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.889.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.917.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.918.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.932.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.935.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.935.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.965.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.969.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.974.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.988.\n",
      "[INFO]\tIn 20000 iteration, current mean episode reward is 1.0.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "q_learning_trainer = q_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== The state value for action 0 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.262|0.262|0.328|0.410|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.328|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.410|0.410|0.512|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.000|0.640|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 1 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.328|0.000|0.512|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.410|0.000|0.640|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.000|0.640|0.800|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.640|0.800|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 2 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.328|0.410|0.328|0.328|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.000|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.512|0.640|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.800|1.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 3 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.262|0.328|0.410|0.328|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.262|0.000|0.410|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.328|0.000|0.512|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.512|0.640|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "q_learning_trainer.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "q_learning_trainer.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have finished Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Monte Carlo Control\n",
    "(40/100 points)\n",
    "\n",
    "In sections 1 and 2, we implement the on-policy and off-policy versions of the TD Learning algorithms. In this section, we will play with another branch of the model-free algorithm: Monte Carlo Control. You can refer to the 5.3 Monte Carlo Control section of the textbook \"Reinforcement Learning: An Introduction\" to learn the details of MC control.\n",
    "\n",
    "The basic idea of MC control is to compute the Q value (state-action value) directly from an episode, without using TD to fit the Q function. Concretely, we maintain a batch of lists (the total number of lists is `obs_dim * action_dim`), each elememnt of the batch is a list correspondent to a state-action pair. The list is used to store the previously happenning \"return\" of each state action pair.\n",
    "\n",
    "We will use a dict `self.returns` to store all lists. The keys of the dict are tuples `(obs, act)`: `self.returns[(obs, act)]` is the list to store all returns when `(obs, act)` happens. \n",
    "\n",
    "The key point of MC Control method is that we take the mean of this list (the mean of all previous returns) as the Q value of this state-action pair.\n",
    "\n",
    "The \"return\" here is the discounted return starting from the state-action pair: $Return(s_t, a_t) = \\sum_{i=0} \\gamma^{t+i} r_{t+i}$.\n",
    "\n",
    "In short, MC Control method uses a new way to estimate the values of state-action pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "class MCControlTrainer(TabularRLTrainerAbstract):\n",
    "    def __init__(self,\n",
    "                 gamma=1.0,\n",
    "                 eps=0.3,\n",
    "                 max_episode_length=100,\n",
    "                 env_name='FrozenLake8x8-v0'\n",
    "                 ):\n",
    "        super(MCControlTrainer, self).__init__(env_name, model_based=False)\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.max_episode_length = max_episode_length\n",
    "\n",
    "        # build the dict of lists\n",
    "        self.returns = {}\n",
    "        for obs in range(self.obs_dim):\n",
    "            for act in range(self.action_dim):\n",
    "                self.returns[(obs, act)] = []\n",
    "\n",
    "        # build the Q table\n",
    "        self.table = np.zeros((self.obs_dim, self.action_dim))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        \"\"\"Implement epsilon-greedy policy\n",
    "\n",
    "        It is a function that take an integer (state / observation)\n",
    "        as input and return an interger (action).\n",
    "        \"\"\"\n",
    "        p = np.random.random_sample()\n",
    "        if p <= self.eps:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            return np.argmax(self.table[obs,:])\n",
    "            \n",
    "    def train(self):\n",
    "        \"\"\"Conduct one iteration of learning.\"\"\"\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "        obs = self.env.reset()\n",
    "        self.eps = 1\n",
    "\n",
    "        for t in range(self.max_episode_length):\n",
    "            \n",
    "            # Gradually reduce epsilon to the minimum: first exploration, then exploitation \n",
    "            if self.eps > 0.1:\n",
    "                self.eps -= 0.01\n",
    "                \n",
    "            act = self.policy(obs)\n",
    "            next_obs, reward, done, _ = self.env.step(act)\n",
    "            \n",
    "            observations.append(obs)\n",
    "            actions.append(act)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            obs = next_obs\n",
    "\n",
    "            if done: \n",
    "                break\n",
    "\n",
    "        assert len(actions) == len(observations)\n",
    "        assert len(actions) == len(rewards)\n",
    "\n",
    "        occured_state_action_pair = set()\n",
    "        length = len(actions)\n",
    "        g = 0.0\n",
    "        for i in reversed(range(length)):\n",
    "            # if length = 10, then i = 9, 8, ..., 0\n",
    "\n",
    "            obs = observations[i]\n",
    "            act = actions[i]\n",
    "            reward = rewards[i]\n",
    "\n",
    "            g = self.gamma * g + reward\n",
    "\n",
    "            if (obs, act) not in occured_state_action_pair:\n",
    "                occured_state_action_pair.add((obs, act))\n",
    "\n",
    "                self.returns[(obs, act)].append(g)\n",
    "                \n",
    "                self.table[obs, act] = np.average(self.returns[(obs, act)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Managing configurations of your experiments is important for your research.\n",
    "default_mc_control_config = dict(\n",
    "    max_iteration=10000,\n",
    "    max_episode_length=200,\n",
    "    evaluate_interval=1000,\n",
    "    gamma=0.9,\n",
    "    eps=0.3,\n",
    "    env_name='FrozenLakeNotSlippery-v0'\n",
    ")\n",
    "\n",
    "\n",
    "def mc_control(train_config=None):\n",
    "    config = default_mc_control_config.copy()\n",
    "    if train_config is not None:\n",
    "        config.update(train_config)\n",
    "\n",
    "    trainer = MCControlTrainer(\n",
    "        gamma=config['gamma'],\n",
    "        eps=config['eps'],\n",
    "        max_episode_length=config['max_episode_length'],\n",
    "        env_name=config['env_name']\n",
    "    )\n",
    "\n",
    "    for i in range(1, config['max_iteration'] + 1):\n",
    "        # train the agent\n",
    "        trainer.train()\n",
    "\n",
    "        # evaluate the result\n",
    "        if i % config['evaluate_interval'] == 0:\n",
    "            \n",
    "            # gradually reduce epsilon to 0 before each evalution\n",
    "            trainer.eps = 1e-2 * (config['max_iteration'] - i) / config['evaluate_interval']\n",
    "            \n",
    "            print(\n",
    "                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n",
    "                \"\".format(i, trainer.evaluate()))\n",
    "    \n",
    "    # to make sure the evalute runs normally...\n",
    "    trainer.eps = 0\n",
    "    if trainer.evaluate() < 0.6:\n",
    "        print(\"We expect to get the mean episode reward greater than 0.6. \" \\\n",
    "        \"But you get: {}. Please check your codes.\".format(trainer.evaluate()))\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.908.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.91.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.927.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.938.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.956.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.967.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.964.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.974.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.993.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 1.0.\n",
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.792.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.804.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.794.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.813.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.837.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.837.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.864.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.882.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.898.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.904.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.904.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.922.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.92.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.932.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.953.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.962.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.964.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.975.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.648.\n",
      "[INFO]\tIn 20000 iteration, current mean episode reward is 1.0.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "mc_control_trainer = mc_control()\n",
    "\n",
    "sarsa_trainer = sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== The state value for action 0 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.017|0.017|0.019|0.049|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.023|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.048|0.045|0.113|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.000|0.274|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 1 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.020|0.000|0.062|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.046|0.000|0.195|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.000|0.221|0.560|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.230|0.576|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 2 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.013|0.029|0.023|0.032|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.000|0.000|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.117|0.191|0.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.561|1.000|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n",
      "\n",
      "=== The state value for action 3 ===\n",
      "+-----+-----+-----+-----+-----+\n",
      "|     |   0 |   1 |   2 |   3 |\n",
      "|-----+-----+-----+-----+-----+\n",
      "| 0   |0.016|0.016|0.030|0.026|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 1   |0.020|0.000|0.037|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 2   |0.025|0.000|0.055|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "| 3   |0.000|0.119|0.234|0.000|\n",
      "|     |     |     |     |     |\n",
      "+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "mc_control_trainer.print_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "mc_control_trainer.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secion 4 Bonus (optional): Tune and train FrozenLake8x8-v0 with Model-free algorithms\n",
    "\n",
    "You have noticed that we use a simpler environment `FrozenLakeNotSlippery-v0` which has only 16 states and is not stochastic. Can you try to train Model-free families of algorithm using the `FrozenLake8x8-v0` environment? Tune the hyperparameters and compare the results between different algorithms.\n",
    "\n",
    "Hint: It's not easy to train model-free algorithm in `FrozenLake8x8-v0`. Failure is excepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.033.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.051.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.068.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.172.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.147.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.18.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.4.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.141.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.373.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.253.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.109.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.13.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.161.\n",
      "[INFO]\tIn 20000 iteration, current mean episode reward is 0.599.\n",
      "We expect to get the mean episode reward greater than 0.6. But you get: 0.599. Please check your codes.\n"
     ]
    }
   ],
   "source": [
    "# It's ok to leave this cell commented.\n",
    "\n",
    "new_config = dict(\n",
    "    env_name=\"FrozenLake8x8-v0\"\n",
    ")\n",
    "\n",
    "# new_mc_control_trainer = mc_control(new_config)\n",
    "new_q_learning_trainer = q_learning(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "new_q_learning_trainer.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\tIn 1000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 2000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 3000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 4000 iteration, current mean episode reward is 0.0.\n",
      "[INFO]\tIn 5000 iteration, current mean episode reward is 0.039.\n",
      "[INFO]\tIn 6000 iteration, current mean episode reward is 0.129.\n",
      "[INFO]\tIn 7000 iteration, current mean episode reward is 0.244.\n",
      "[INFO]\tIn 8000 iteration, current mean episode reward is 0.385.\n",
      "[INFO]\tIn 9000 iteration, current mean episode reward is 0.161.\n",
      "[INFO]\tIn 10000 iteration, current mean episode reward is 0.325.\n",
      "[INFO]\tIn 11000 iteration, current mean episode reward is 0.423.\n",
      "[INFO]\tIn 12000 iteration, current mean episode reward is 0.539.\n",
      "[INFO]\tIn 13000 iteration, current mean episode reward is 0.216.\n",
      "[INFO]\tIn 14000 iteration, current mean episode reward is 0.476.\n",
      "[INFO]\tIn 15000 iteration, current mean episode reward is 0.547.\n",
      "[INFO]\tIn 16000 iteration, current mean episode reward is 0.53.\n",
      "[INFO]\tIn 17000 iteration, current mean episode reward is 0.449.\n",
      "[INFO]\tIn 18000 iteration, current mean episode reward is 0.3.\n",
      "[INFO]\tIn 19000 iteration, current mean episode reward is 0.647.\n",
      "[INFO]\tIn 20000 iteration, current mean episode reward is 0.434.\n",
      "We expect to get the mean episode reward greater than 0.6. But you get: 0.434. Please check your codes.\n"
     ]
    }
   ],
   "source": [
    "new_sarsa_trainer = sarsa(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "new_sarsa_trainer.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have implement the MC Control algorithm. You have finished this section. If you want to do more investigation like comparing the policy provided by SARSA, Q-Learning and MC Control, then you can do it in the next cells. It's OK to leave it blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n2. Monte-Carlo vs TD\\n\\nEpisode-by-episode instead of step-by-step.\\n\\nTheoretically, if there're enough episodes, MC will get the perfect solution,\\nhence the speed of MC is apparently slower than TD's.\\n\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparisons:\n",
    "\n",
    "'''\n",
    "\n",
    "1. SARSA vs Q-Learning\n",
    "\n",
    "The only difference is the \"next action choosing policy\":\n",
    "SARSA is epsilon-greedy while Q-Learning is absolute greedy.\n",
    "\n",
    "Therefore, when convergence, SARSA's policy is more conservative,\n",
    "while Q-Learning's policy is more bold and radical, and of course,\n",
    "usually \"better\" (with less time/cost/distance).\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "2. Monte-Carlo vs TD\n",
    "\n",
    "Episode-by-episode instead of step-by-step.\n",
    "\n",
    "Theoretically, if there're enough episodes, MC will get the perfect solution,\n",
    "hence the speed of MC is apparently slower than TD's.\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Conclusion and Discussion\n",
    "\n",
    "It's OK to leave the following cells empty. In the next markdown cell, you can write whatever you like. Like the suggestion on the course, the confusing problems in the assignments, and so on.\n",
    "\n",
    "If you want to do more investigation, feel free to open new cells via `Esc + B` after the next cells and write codes in it, so that you can reuse some result in this notebook. Remember to write sufficient comments and documents to let others know what you are doing.\n",
    "\n",
    "Following the submission instruction in the assignment to submit your assignment to our staff. Thank you!\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
